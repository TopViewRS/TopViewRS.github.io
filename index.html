<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners.">
  <meta name="keywords" content="TOPVIEWRS">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners.</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ltl.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- 
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
 -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chengzu-li.github.io/">Chengzu Li</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://caiqizh.github.io/">Caiqi Zhang</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://hzhou.top/">Han Zhou</a><sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/nhcollier/">Nigel Collier</a><sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/annakorhonen/">Anna Korhonen</a><sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/ivanvulic/">Ivan Vulić</a><sup></sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup></sup>University of Cambridge</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.02537"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.02537"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/cambridgeltl/topviewrs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/cambridgeltl/topviewrs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
      </h2>
      <img src="static/images/front_fig.png"> <!-- To .gif format-->
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <p><strong>1)</strong> We define the top-view spatial reasoning challenge for VLMs via 4 carefully designed tasks of increasing complexity, also encompassing 9 distinct fine-grained sub-tasks with a structured design of the questions focusing on different model abilities.</p>
          <p><strong>2)</strong> We collect the <strong>TopViewRS</strong> dataset, comprising 11,384 multiple-choice questions with either photo-realistic or semantic top-view maps of real-world scenarios through a pipeline of automatic collection followed by human alignment.</p>
          <p><strong>3)</strong> We use <strong>TopViewRS</strong> to evaluate and study 10 VLMs from different model families and sizes, highlighting the performance gap compared to human annotators.</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Top-view perspective denotes a typical way in which humans read and reason over different types of maps, 
            and it is vital for localization and navigation of humans as well as of 
            <em>non-human</em> agents, such as the ones backed by large Vision-Language Models (VLMs). 
            Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored. 
            In this work, we thus study their capability to understand and reason over spatial relations from the top view. 
            The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; 
            we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). 
            We introduce the <strong>TopViewRS</strong> 
            (<strong>Top</strong>-<strong>View</strong> <strong>R</strong>easoning in <strong>S</strong>pace) dataset, 
            consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. 
            We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. 
            Evaluation of 10 representative open- and closed-source VLMs reveals the <em>gap</em> of more than 50% compared to average human performance, 
            and it is even <em>lower</em> than the random baseline in some cases. 
            Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, 
            the overall performance of VLMs remains limited. 
            Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning 
            and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Tasks and Sub-Tasks</h2>
        <div class="content has-text-justified">
          <p>We define 4 different tasks which cover a total of 9 finer-grained sub-tasks. The tasks are designed to have an increasing level of complexity, where each subsequent task depends on the abilities measured in the preceding one(s).</p>

          <p><strong>(1) Top-View Recognition</strong> evaluates the fundamental ability to interpret the input map, and covers two sub-tasks: <i>Object Recognition</i> and <i>Scene Recognition</i>. It does not require the model to identify specific locations of objects and rooms.</p>
          <img src="static/images/top-view-recognition.png">
          <br><br>
          <p><strong>(2) Top-View Localization</strong> investigates whether the model can localize objects or rooms in the top-view map based on textual descriptions, including <i>Object Localization</i> and <i>Scene Localization</i> as two sub-tasks. Beyond understanding the top-view map as a whole, it requires the model to ground entities in the map, representing the model's ability to align spatial descriptions with corresponding locations.</p>
          <img src="static/images/top-view-localization.png">
          <br><br>
          <p><strong>(3) Static Spatial Reasoning</strong> aims to evaluate the model's spatial reasoning ability with more complex questions. It includes two sub-tasks: reasoning over <i>Scene Counting</i> and <i>Relative Spatial Relations</i> between different objects and rooms. These questions require the model to perform multi-step reasoning based on the recognition and localization of entities in the top-view map.</p>
          <img src="static/images/static-spatial-reasoning.png">
          <br><br>
          <p><strong>(4) Dynamic Spatial Reasoning</strong> Finally, we introduce a novel task that involves dynamic spatial reasoning over top-view maps in the context of agent navigation. It requires the model to understand the sequential relations along the points of the navigation path (sub-task <i>Dynamic Action Counting</i>) and answer spatial questions with regard to the dynamic navigation path (sub-task <i>Dynamic Relative Spatial Reasoning</i>) and the circumstantial environments (<i>Dynamic Spatial Localization</i>).</p>
          <img src="static/images/dynamic-spatial-reasoning.png">
          <br><br>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment Results</h2>
        <div class="content has-text-justified">
          <img src="static/images/main_table.png"> <!-- To .gif format-->
          <img src="static/images/main_visualization.png"> <!-- To .gif format-->
          <br><br>
          <div class="key-findings">
            <h3 class="title is-4">Key Findings:</h3>
            <ul>
              <li><strong> Models perform better on recognition and localization tasks compared to reasoning tasks.</strong> Top-View Recognition consistently demonstrates the highest performance across all models. Gemini shows human-comparable performance with over 90% EM score.</li>
              <li><strong> Larger models do not always show better spatial awareness.</strong> Meanwhile, closed-source models outperform open-source models in easier tasks but the performance gap narrows as the task complexity increases.</li>
              <li><strong> Models perform better in easier tasks with semantic maps, compared with realistic maps. </strong> However, for Top-View Localization and Static Spatial Reasoning, models struggle to utilize semantic top-view maps, yielding performances akin to random baselines in both EM and PM accuracy.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Discussion</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Gap to Human Performance</h3>
          <div style="text-align: center; margin: 1em 0;">
            <img src="static/images/human.png" style="width: 60%; height: auto;" text-align: center;>
          </div>
          <p>We evaluated human performance on a dataset of 60 top-view maps, with four participants achieving 90% accuracy across all the sub-task. This revealed a significant performance gap, with humans outperforming models by over 50% across all the sub-tasks that involve spatial awareness. </p>
          
          <h3 class="title is-4">Chain-of-Thought Helps Elicit Spatial Reasoning</h3>
          <div style="text-align: center; margin: 1em 0;">
            <img src="static/images/cot.png" style="width: 60%; height: auto;">
          </div>

          <p>We explore Chain-of-Thought (CoT) reasoning for our tasks, where we ask models first localize entities before answering. Experiments with GPT-4V and Gemini showed a performance increase of 6.50% on realistic maps and 5.14% on semantic maps, highlighting the effectiveness of step-by-step reasoning.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{li2024topviewrs,
        title={TopViewRS: Vision-Language Models as Top-View Spatial Reasoners},
        author={Chengzu Li and Caiqi Zhang and Han Zhou and Nigel Collier and Anna Korhonen and Ivan Vulić},
        year={2024},
        eprint={2406.02537},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
    }
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
